{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11483716,"sourceType":"datasetVersion","datasetId":7197496},{"sourceId":11511455,"sourceType":"datasetVersion","datasetId":7218377},{"sourceId":11597739,"sourceType":"datasetVersion","datasetId":7273174},{"sourceId":11635702,"sourceType":"datasetVersion","datasetId":7300625}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport pandas as pd\nfrom skimage import io, transform\nfrom torch.utils.data import Dataset\nimport cv2\nfrom torch.utils.data import DataLoader\nfrom skimage.color import rgb2gray","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T08:42:55.315026Z","iopub.execute_input":"2025-05-01T08:42:55.315604Z","iopub.status.idle":"2025-05-01T08:42:57.450082Z","shell.execute_reply.started":"2025-05-01T08:42:55.315582Z","shell.execute_reply":"2025-05-01T08:42:57.449511Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"**U2NET functions**___","metadata":{}},{"cell_type":"code","source":"def normalize_prediction(image_tensor):\n    \"\"\"Normalize the predicted image tensor to the range [0, 1].\"\"\"\n    image_num = image_tensor.size(0)\n    image_tensor = image_tensor.clone().detach()\n    image_min = torch.min(image_tensor.view(image_num, -1), dim=1)[0]\n    image_max = torch.max(image_tensor.view(image_num, -1), dim=1)[0]\n    image_tensor = (image_tensor - image_min[:, None, None, None]) / (image_max[:, None, None, None] - image_min[:, None, None, None])\n    return image_tensor\n\ndef save_images(image_tensor, mask_paths, save_path):\n    \"\"\"Save images after normalization, resizing them to match the original mask size.\"\"\"\n    image_num = image_tensor.size(0)\n    images = (normalize_prediction(image_tensor) * 255).clone().detach().permute(0, 2, 3, 1).cpu().numpy()\n\n    for i in range(image_num):\n        mask_shape = cv2.imread(mask_paths[i]).shape[:2]\n        resized_image = cv2.resize(images[i], dsize=(mask_shape[1], mask_shape[0]), interpolation=cv2.INTER_LINEAR)\n        save_filename = os.path.join(save_path, os.path.basename(mask_paths[i]))\n        cv2.imwrite(save_filename, resized_image)\n\ndef calculate_iou_and_save(image_tensor, target_tensor, mask_paths, save_path):\n    \"\"\"Calculate Intersection over Union (IoU) and save results.\"\"\"\n    pred = 1 - torch.round(image_tensor.clone().detach()).long()\n    target = 1 - torch.round(target_tensor.clone().detach()).long()\n\n    intersection = torch.sum(pred & target, dim=(1, 2, 3)).float()\n    union = torch.sum(pred | target, dim=(1, 2, 3)).float()\n    ious = intersection / union\n\n    txt_path = os.path.join(save_path, 'iou.txt')\n    new_data = np.c_[np.array([os.path.basename(x) for x in mask_paths]), ious.detach().cpu().numpy()]\n\n    if os.path.isfile(txt_path):\n        existing_data = pd.read_table(txt_path, sep=' ', header=None).values\n        new_data = np.r_[existing_data, new_data]\n\n    pd.DataFrame(new_data).to_csv(txt_path, sep=' ', index=False, header=False)\n\n    return ious.mean().item()\n    \ndef calculate_bce_loss(d_list, target):\n    \"\"\"Calculate the BCE loss across multiple U-Net outputs.\"\"\"\n    criterion = nn.BCELoss()\n    losses = [criterion(d, target) for d in d_list]\n    total_loss = sum(losses)\n    return losses[0], total_loss, *losses[1:]\n\nclass SegmentationDataset(Dataset):\n    \"\"\"Custom Dataset class for loading images and masks.\"\"\"\n    def __init__(self, data_path, mask_path, resize=512, data_postfix='.jpg', mask_postfix='.jpg'):\n        self.data_paths = self.get_file_paths(data_path, data_postfix)\n        self.mask_paths = self.get_file_paths(mask_path, mask_postfix, self.data_paths)\n        self.resize = resize\n\n    def __getitem__(self, idx):\n        image = io.imread(self.data_paths[idx])[:, :, :3]\n        image = transform.resize(image, (self.resize, self.resize), mode='constant') / np.max(image)\n        \n        mask = io.imread(self.mask_paths[idx])\n        if len(mask.shape) == 2:  # Grayscale mask\n            mask_resized = transform.resize(mask, (self.resize, self.resize), mode='constant', order=0, preserve_range=True)\n        else:  # RGB mask, use the first channel\n            mask_resized = transform.resize(rgb2gray(mask), (self.resize, self.resize), mode='constant', order=0, preserve_range=True)\n\n        mask_resized = mask_resized / np.max(mask_resized)\n        image = (image - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])  # Normalize image\n\n        return torch.tensor(image, dtype=torch.float).permute(2, 0, 1), torch.tensor(mask_resized, dtype=torch.float).unsqueeze(0), self.mask_paths[idx]\n\n    def __len__(self):\n        return len(self.data_paths)\n\n    @staticmethod\n    def get_file_paths(path, postfix, reference_list=None):\n        \"\"\"Get the file paths for the dataset.\"\"\"\n        root = os.getcwd()\n        file_list = []\n\n        if reference_list is None:\n            for file in os.listdir(path):\n                if os.path.splitext(file)[-1] == postfix:\n                    file_list.append(os.path.join(root, path, file))\n        else:\n            for ref in reference_list:\n                file_list.append(os.path.join(root, path, os.path.splitext(os.path.split(ref)[1])[0] + postfix))\n\n        return file_list","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T08:42:59.761681Z","iopub.execute_input":"2025-05-01T08:42:59.762650Z","iopub.status.idle":"2025-05-01T08:42:59.777212Z","shell.execute_reply.started":"2025-05-01T08:42:59.762625Z","shell.execute_reply":"2025-05-01T08:42:59.776423Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"**U2net**__","metadata":{}},{"cell_type":"code","source":"class REBNCONV(nn.Module):\n    def __init__(self, in_ch=3, out_ch=3, dilation=1):\n        super(REBNCONV, self).__init__()\n        self.layer = nn.Sequential(\n            nn.Conv2d(in_ch, out_ch, 3, padding=dilation, dilation=dilation),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        return self.layer(x)\n\n\ndef _upsample_(src, tar):\n    return nn.functional.interpolate(src, size=tar.shape[2:], mode='bilinear', align_corners=True)\n\n\nclass RSU1(nn.Module):\n    def __init__(self, in_ch=3, inner_ch=12, out_ch=3):\n        super(RSU1, self).__init__()\n        self.rebnconvin = REBNCONV(in_ch, out_ch, dilation=1)\n\n        self.rebnconv1 = REBNCONV(out_ch, inner_ch, dilation=1)\n\n        self.pool1 = nn.MaxPool2d(2, stride=2, ceil_mode=True)\n        self.rebnconv2 = REBNCONV(inner_ch, inner_ch, dilation=1)\n\n        self.pool2 = nn.MaxPool2d(2, stride=2, ceil_mode=True)\n        self.rebnconv3 = REBNCONV(inner_ch, inner_ch, dilation=1)\n\n        self.pool3 = nn.MaxPool2d(2, stride=2, ceil_mode=True)\n        self.rebnconv4 = REBNCONV(inner_ch, inner_ch, dilation=1)\n\n        self.pool4 = nn.MaxPool2d(2, stride=2, ceil_mode=True)\n        self.rebnconv5 = REBNCONV(inner_ch, inner_ch, dilation=1)\n\n        self.pool5 = nn.MaxPool2d(2, stride=2, ceil_mode=True)\n        self.rebnconv6 = REBNCONV(inner_ch, inner_ch, dilation=1)\n\n        self.rebnconv7 = REBNCONV(inner_ch, inner_ch, dilation=2)\n\n        self.rebnconv6d = REBNCONV(inner_ch * 2, inner_ch, dilation=1)\n        self.rebnconv5d = REBNCONV(inner_ch * 2, inner_ch, dilation=1)\n        self.rebnconv4d = REBNCONV(inner_ch * 2, inner_ch, dilation=1)\n        self.rebnconv3d = REBNCONV(inner_ch * 2, inner_ch, dilation=1)\n        self.rebnconv2d = REBNCONV(inner_ch * 2, inner_ch, dilation=1)\n        self.rebnconv1d = REBNCONV(inner_ch * 2, out_ch, dilation=1)\n\n    def forward(self, x):\n        hxin = self.rebnconvin(x)\n        hx1 = self.rebnconv1(hxin)\n\n        x = self.pool1(hx1)\n        hx2 = self.rebnconv2(x)\n\n        x = self.pool2(hx2)\n        hx3 = self.rebnconv3(x)\n\n        x = self.pool3(hx3)\n        hx4 = self.rebnconv4(x)\n\n        x = self.pool4(hx4)\n        hx5 = self.rebnconv5(x)\n\n        x = self.pool5(hx5)\n        hx6 = self.rebnconv6(x)\n\n        hx7 = self.rebnconv7(hx6)\n\n        hx6d = self.rebnconv6d(torch.cat((hx7, hx6), dim=1))\n\n        hx6dup = _upsample_(hx6d, hx5)\n        hx5d = self.rebnconv5d(torch.cat((hx6dup, hx5), dim=1))\n\n        hx5dup = _upsample_(hx5d, hx4)\n        hx4d = self.rebnconv4d(torch.cat((hx5dup, hx4), dim=1))\n\n        hx4dup = _upsample_(hx4d, hx3)\n        hx3d = self.rebnconv3d(torch.cat((hx4dup, hx3), dim=1))\n\n        hx3dup = _upsample_(hx3d, hx2)\n        hx2d = self.rebnconv2d(torch.cat((hx3dup, hx2), dim=1))\n\n        hx2dup = _upsample_(hx2d, hx1)\n        hx1d = self.rebnconv1d(torch.cat((hx2dup, hx1), dim=1))\n\n        return hx1d + hxin\n\n\nclass RSU2(nn.Module):\n    def __init__(self, in_ch=3, inner_ch=12, out_ch=3):\n        super(RSU2, self).__init__()\n        self.rebnconvin = REBNCONV(in_ch, out_ch, dilation=1)\n\n        self.rebnconv1 = REBNCONV(out_ch, inner_ch, dilation=1)\n\n        self.pool1 = nn.MaxPool2d(2, stride=2, ceil_mode=True)\n        self.rebnconv2 = REBNCONV(inner_ch, inner_ch, dilation=1)\n\n        self.pool2 = nn.MaxPool2d(2, stride=2, ceil_mode=True)\n        self.rebnconv3 = REBNCONV(inner_ch, inner_ch, dilation=1)\n\n        self.pool3 = nn.MaxPool2d(2, stride=2, ceil_mode=True)\n        self.rebnconv4 = REBNCONV(inner_ch, inner_ch, dilation=1)\n\n        self.pool4 = nn.MaxPool2d(2, stride=2, ceil_mode=True)\n        self.rebnconv5 = REBNCONV(inner_ch, inner_ch, dilation=1)\n\n        self.rebnconv6 = REBNCONV(inner_ch, inner_ch, dilation=2)\n\n        self.rebnconv5d = REBNCONV(inner_ch * 2, inner_ch, dilation=1)\n        self.rebnconv4d = REBNCONV(inner_ch * 2, inner_ch, dilation=1)\n        self.rebnconv3d = REBNCONV(inner_ch * 2, inner_ch, dilation=1)\n        self.rebnconv2d = REBNCONV(inner_ch * 2, inner_ch, dilation=1)\n        self.rebnconv1d = REBNCONV(inner_ch * 2, out_ch, dilation=1)\n\n    def forward(self, x):\n        hxin = self.rebnconvin(x)\n        hx1 = self.rebnconv1(hxin)\n\n        x = self.pool1(hx1)\n        hx2 = self.rebnconv2(x)\n\n        x = self.pool2(hx2)\n        hx3 = self.rebnconv3(x)\n\n        x = self.pool3(hx3)\n        hx4 = self.rebnconv4(x)\n\n        x = self.pool4(hx4)\n        hx5 = self.rebnconv5(x)\n\n        hx6 = self.rebnconv6(hx5)\n\n        hx5d = self.rebnconv5d(torch.cat((hx6, hx5), dim=1))\n\n        hx5dup = _upsample_(hx5d, hx4)\n        hx4d = self.rebnconv4d(torch.cat((hx5dup, hx4), dim=1))\n\n        hx4dup = _upsample_(hx4d, hx3)\n        hx3d = self.rebnconv3d(torch.cat((hx4dup, hx3), dim=1))\n\n        hx3dup = _upsample_(hx3d, hx2)\n        hx2d = self.rebnconv2d(torch.cat((hx3dup, hx2), dim=1))\n\n        hx2dup = _upsample_(hx2d, hx1)\n        hx1d = self.rebnconv1d(torch.cat((hx2dup, hx1), dim=1))\n\n        return hx1d + hxin\n\n\nclass RSU3(nn.Module):\n    def __init__(self, in_ch=3, inner_ch=12, out_ch=3):\n        super(RSU3, self).__init__()\n        self.rebnconvin = REBNCONV(in_ch, out_ch, dilation=1)\n\n        self.rebnconv1 = REBNCONV(out_ch, inner_ch, dilation=1)\n\n        self.pool1 = nn.MaxPool2d(2, stride=2, ceil_mode=True)\n        self.rebnconv2 = REBNCONV(inner_ch, inner_ch, dilation=1)\n\n        self.pool2 = nn.MaxPool2d(2, stride=2, ceil_mode=True)\n        self.rebnconv3 = REBNCONV(inner_ch, inner_ch, dilation=1)\n\n        self.pool3 = nn.MaxPool2d(2, stride=2, ceil_mode=True)\n        self.rebnconv4 = REBNCONV(inner_ch, inner_ch, dilation=1)\n\n        # self.pool4 = nn.MaxPool2d(2, stride=2, ceil_mode=True)\n        # self.rebnconv5 = REBNCONV(inner_ch, inner_ch, dilation=1)\n\n        self.rebnconv6 = REBNCONV(inner_ch, inner_ch, dilation=2)\n\n        # self.rebnconv5d = REBNCONV(inner_ch * 2, inner_ch, dilation=1)\n        self.rebnconv4d = REBNCONV(inner_ch * 2, inner_ch, dilation=1)\n        self.rebnconv3d = REBNCONV(inner_ch * 2, inner_ch, dilation=1)\n        self.rebnconv2d = REBNCONV(inner_ch * 2, inner_ch, dilation=1)\n        self.rebnconv1d = REBNCONV(inner_ch * 2, out_ch, dilation=1)\n\n    def forward(self, x):\n        hxin = self.rebnconvin(x)\n        hx1 = self.rebnconv1(hxin)\n\n        x = self.pool1(hx1)\n        hx2 = self.rebnconv2(x)\n\n        x = self.pool2(hx2)\n        hx3 = self.rebnconv3(x)\n\n        x = self.pool3(hx3)\n        hx4 = self.rebnconv4(x)\n\n        hx5 = self.rebnconv6(hx4)\n\n        hx4d = self.rebnconv4d(torch.cat((hx5, hx4), dim=1))\n\n        hx4dup = _upsample_(hx4d, hx3)\n        hx3d = self.rebnconv3d(torch.cat((hx4dup, hx3), dim=1))\n\n        hx3dup = _upsample_(hx3d, hx2)\n        hx2d = self.rebnconv2d(torch.cat((hx3dup, hx2), dim=1))\n\n        hx2dup = _upsample_(hx2d, hx1)\n        hx1d = self.rebnconv1d(torch.cat((hx2dup, hx1), dim=1))\n\n        return hx1d + hxin\n\n\nclass RSU4(nn.Module):\n    def __init__(self, in_ch=3, inner_ch=12, out_ch=3):\n        super(RSU4, self).__init__()\n        self.rebnconvin = REBNCONV(in_ch, out_ch, dilation=1)\n\n        self.rebnconv1 = REBNCONV(out_ch, inner_ch, dilation=1)\n\n        self.pool1 = nn.MaxPool2d(2, stride=2, ceil_mode=True)\n        self.rebnconv2 = REBNCONV(inner_ch, inner_ch, dilation=1)\n\n        self.pool2 = nn.MaxPool2d(2, stride=2, ceil_mode=True)\n        self.rebnconv3 = REBNCONV(inner_ch, inner_ch, dilation=1)\n\n        self.rebnconv6 = REBNCONV(inner_ch, inner_ch, dilation=2)\n\n        self.rebnconv3d = REBNCONV(inner_ch * 2, inner_ch, dilation=1)\n        self.rebnconv2d = REBNCONV(inner_ch * 2, inner_ch, dilation=1)\n        self.rebnconv1d = REBNCONV(inner_ch * 2, out_ch, dilation=1)\n\n    def forward(self, x):\n        hxin = self.rebnconvin(x)\n        hx1 = self.rebnconv1(hxin)\n\n        x = self.pool1(hx1)\n        hx2 = self.rebnconv2(x)\n\n        x = self.pool2(hx2)\n        hx3 = self.rebnconv3(x)\n\n        hx4 = self.rebnconv6(hx3)\n\n        hx3d = self.rebnconv3d(torch.cat((hx4, hx3), dim=1))\n\n        hx3dup = _upsample_(hx3d, hx2)\n        hx2d = self.rebnconv2d(torch.cat((hx3dup, hx2), dim=1))\n\n        hx2dup = _upsample_(hx2d, hx1)\n        hx1d = self.rebnconv1d(torch.cat((hx2dup, hx1), dim=1))\n\n        return hx1d + hxin\n\n\nclass RSU5F(nn.Module):\n    def __init__(self, in_ch=3, inner_ch=12, out_ch=3):\n        super(RSU5F, self).__init__()\n        self.rebnconvin = REBNCONV(in_ch, out_ch, dilation=1)\n        self.rebnconv1 = REBNCONV(out_ch, inner_ch, dilation=1)\n        self.rebnconv2 = REBNCONV(inner_ch, inner_ch, dilation=2)\n        self.rebnconv3 = REBNCONV(inner_ch, inner_ch, dilation=4)\n\n        self.rebnconv4 = REBNCONV(inner_ch, inner_ch, dilation=8)\n\n        self.rebnconv3d = REBNCONV(inner_ch * 2, inner_ch, dilation=4)\n        self.rebnconv2d = REBNCONV(inner_ch * 2, inner_ch, dilation=2)\n        self.rebnconv1d = REBNCONV(inner_ch * 2, out_ch, dilation=1)\n\n    def forward(self, x):\n        hxin = self.rebnconvin(x)\n\n        hx1 = self.rebnconv1(hxin)\n        hx2 = self.rebnconv2(hx1)\n        hx3 = self.rebnconv3(hx2)\n\n        hx4 = self.rebnconv4(hx3)\n\n        hx3d = self.rebnconv3d(torch.cat((hx4, hx3), dim=1))\n        hx2d = self.rebnconv2d(torch.cat((hx3d, hx2), dim=1))\n        hx1d = self.rebnconv1d(torch.cat((hx2d, hx1), dim=1))\n\n        return hx1d + hxin\n\nclass U2NET(nn.Module):\n    def __init__(self, in_ch=3, out_ch=1):\n        super(U2NET, self).__init__()\n        self.encoder1 = RSU1(in_ch, 32, 64)\n        self.pool1 = nn.MaxPool2d(2, stride=2, ceil_mode=True)\n\n        self.encoder2 = RSU2(64, 32, 128)\n        self.pool2 = nn.MaxPool2d(2, stride=2, ceil_mode=True)\n\n        self.encoder3 = RSU3(128, 64, 256)\n        self.pool3 = nn.MaxPool2d(2, stride=2, ceil_mode=True)\n\n        self.encoder4 = RSU4(256, 128, 512)\n        self.pool4 = nn.MaxPool2d(2, stride=2, ceil_mode=True)\n\n        self.encoder5 = RSU5F(512, 256, 512)\n        self.pool5 = nn.MaxPool2d(2, stride=2, ceil_mode=True)\n\n        self.encoder6 = RSU5F(512, 256, 512)\n\n        self.decoder5 = RSU5F(1024, 256, 512)\n        self.decoder4 = RSU4(1024, 128, 256)\n        self.decoder3 = RSU3(512, 64, 128)\n        self.decoder2 = RSU2(256, 32, 64)\n        self.decoder1 = RSU1(128, 16, 64)\n\n        self.side1 = nn.Conv2d(64, out_ch, 3, padding=1)\n        self.side2 = nn.Conv2d(64, out_ch, 3, padding=1)\n        self.side3 = nn.Conv2d(128, out_ch, 3, padding=1)\n        self.side4 = nn.Conv2d(256, out_ch, 3, padding=1)\n        self.side5 = nn.Conv2d(512, out_ch, 3, padding=1)\n        self.side6 = nn.Conv2d(512, out_ch, 3, padding=1)\n\n        self.out_conv = nn.Conv2d(6, out_ch, 1)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        hx1 = self.encoder1(x)\n        x = self.pool1(hx1)\n\n        hx2 = self.encoder2(x)\n        x = self.pool2(hx2)\n\n        hx3 = self.encoder3(x)\n        x = self.pool3(hx3)\n\n        hx4 = self.encoder4(x)\n        x = self.pool4(hx4)\n\n        hx5 = self.encoder5(x)\n        x = self.pool5(hx5)\n\n        hx6 = self.encoder6(x)\n        hx6up = _upsample_(hx6, hx5)\n\n        hx5d = self.decoder5(torch.cat((hx6up, hx5), dim=1))\n        hx5dup = _upsample_(hx5d, hx4)\n\n        hx4d = self.decoder4(torch.cat((hx5dup, hx4), dim=1))\n        hx4dup = _upsample_(hx4d, hx3)\n\n        hx3d = self.decoder3(torch.cat((hx4dup, hx3), dim=1))\n        hx3dup = _upsample_(hx3d, hx2)\n\n        hx2d = self.decoder2(torch.cat((hx3dup, hx2), dim=1))\n        hx2dup = _upsample_(hx2d, hx1)\n\n        hx1d = self.decoder1(torch.cat((hx2dup, hx1), dim=1))\n\n        d1 = self.side1(hx1d)\n\n        d2 = self.side2(hx2d)\n        d2 = _upsample_(d2, d1)\n\n        d3 = self.side3(hx3d)\n        d3 = _upsample_(d3, d1)\n\n        d4 = self.side4(hx4d)\n        d4 = _upsample_(d4, d1)\n\n        d5 = self.side5(hx5d)\n        d5 = _upsample_(d5, d1)\n\n        d6 = self.side6(hx6)\n        d6 = _upsample_(d6, d1)\n\n        d0 = self.out_conv(torch.cat((d1, d2, d3, d4, d5, d6), dim=1))\n\n        return self.sigmoid(d0), self.sigmoid(d1), self.sigmoid(d2), self.sigmoid(d3), self.sigmoid(d4), self.sigmoid(d5), self.sigmoid(d6)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T08:43:03.234108Z","iopub.execute_input":"2025-05-01T08:43:03.234918Z","iopub.status.idle":"2025-05-01T08:43:03.283281Z","shell.execute_reply.started":"2025-05-01T08:43:03.234884Z","shell.execute_reply":"2025-05-01T08:43:03.282470Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import shutil\nshutil.rmtree(\"/kaggle/working/results\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T09:23:57.279120Z","iopub.execute_input":"2025-04-22T09:23:57.279825Z","iopub.status.idle":"2025-04-22T09:23:57.290552Z","shell.execute_reply.started":"2025-04-22T09:23:57.279797Z","shell.execute_reply":"2025-04-22T09:23:57.289930Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Tải & Tiền Xử Lý Dữ Liệu\n# define paths\ntrain_data_path = '/kaggle/input/train-test-u2net/train/images'\ntrain_mask_path = '/kaggle/input/train-test-u2net/train/masks'\ntest_data_path = '/kaggle/input/train-test-u2net/test/images'\ntest_mask_path = '/kaggle/input/train-test-u2net/test/masks'\n# Hyperparameters:\nepochs = 50\nsave_epoch_interval = 10\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# Data loaders\ntrain_loader = DataLoader(SegmentationDataset(train_data_path, train_mask_path), shuffle=True, num_workers=4, batch_size=4)\ntest_loader = DataLoader(SegmentationDataset(test_data_path, test_mask_path), shuffle=True, num_workers=4, batch_size=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T08:48:52.704719Z","iopub.execute_input":"2025-05-01T08:48:52.705308Z","iopub.status.idle":"2025-05-01T08:48:52.765320Z","shell.execute_reply.started":"2025-05-01T08:48:52.705287Z","shell.execute_reply":"2025-05-01T08:48:52.764438Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# 2. Định Nghĩa Mô Hình U2-Net\nmodel = U2NET().to(device=device)\nif torch.cuda.device_count() > 1:\n  model = nn.DataParallel(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T09:12:16.841206Z","iopub.execute_input":"2025-04-22T09:12:16.841770Z","iopub.status.idle":"2025-04-22T09:12:17.330416Z","shell.execute_reply.started":"2025-04-22T09:12:16.841749Z","shell.execute_reply":"2025-04-22T09:12:17.329857Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 3. Biên Dịch Mô Hình\noptimizer = torch.optim.Adam(model.parameters())\nscheduler = None\n\n# Create directory to save model\nos.makedirs('models', exist_ok=True)\n\nlog_interval = 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T09:24:25.618474Z","iopub.execute_input":"2025-04-22T09:24:25.618712Z","iopub.status.idle":"2025-04-22T09:24:25.628099Z","shell.execute_reply.started":"2025-04-22T09:24:25.618696Z","shell.execute_reply":"2025-04-22T09:24:25.627370Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import wandb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T09:29:00.763292Z","iopub.execute_input":"2025-04-22T09:29:00.763779Z","iopub.status.idle":"2025-04-22T09:29:03.140347Z","shell.execute_reply.started":"2025-04-22T09:29:00.763758Z","shell.execute_reply":"2025-04-22T09:29:03.139602Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\n\nmy_secret = user_secrets.get_secret(\"cubi\") \n\nwandb.login(key=my_secret)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T09:31:43.414067Z","iopub.execute_input":"2025-04-22T09:31:43.414590Z","iopub.status.idle":"2025-04-22T09:31:43.704838Z","shell.execute_reply.started":"2025-04-22T09:31:43.414569Z","shell.execute_reply":"2025-04-22T09:31:43.704273Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"wandb.init(\n    project=\"u2net-segmentation\",  # tên tùy bạn đặt\n    name=f\"u2net_run_{epochs}_epochs\",\n    config={\n        \"epochs\": epochs,\n        \"batch_size\": train_loader.batch_size,\n        \"learning_rate\": optimizer.param_groups[0]['lr'],\n        \"resize\": train_loader.dataset.resize,\n        \"model\": \"U2NET\"\n    }\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T09:32:05.663068Z","iopub.execute_input":"2025-04-22T09:32:05.663342Z","iopub.status.idle":"2025-04-22T09:32:13.027991Z","shell.execute_reply.started":"2025-04-22T09:32:05.663323Z","shell.execute_reply":"2025-04-22T09:32:13.027393Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 4. Huấn Luyện, Đánh Giá Mô hình\nprint('-------- Starting Training --------')\n\nfor epoch in range(epochs):\n    model.train()\n    total_loss = 0\n\n    for idx, (images, targets, _) in enumerate(train_loader):\n        images, targets = images.to(device), targets.to(device)\n        # Forward to model\n        d_output = model(images)\n        loss0, loss, *_ = calculate_bce_loss(d_output, targets)\n\n        # Backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item() * images.size(0)\n\n        # Print iteration loss\n        if (idx + 1) % log_interval == 0:  # Log every 'log_interval' iterations\n            print(f'[Epoch {epoch+1}/{epochs}, Iteration {idx+1}/{len(train_loader)}] '\n                  f'Batch Loss: {loss.item():.6f}, '\n                  f'Total Loss: {total_loss / ((idx + 1) * images.size(0)):.6f}')\n\n        # Learning rate scheduler step:\n        if scheduler:\n            scheduler.step(total_loss / len(train_loader.dataset))\n\n    avg_loss = total_loss / len(train_loader)\n    print(f'[Epoch {epoch+1}/{epochs}] Average loss: {total_loss / len(train_loader):.6f}')\n\n    # Log loss lên wandb\n    wandb.log({\n        \"epoch\": epoch + 1,\n        \"train_loss\": avg_loss\n    })\n    \n    # Save model\n    if (epoch + 1) % save_epoch_interval == 0:\n        model_save_path = f'models/u2net_{epoch+1}.pth'\n        torch.save(\n            model.module.state_dict() if torch.cuda.device_count() > 1 else model.state_dict(),\n            model_save_path\n        )\n\n        results_folder = os.path.join('results', f'testing_result_{epoch+1}')\n        os.makedirs(results_folder, exist_ok=True)\n\n        # Evaluation mode\n        model.eval()\n        total_iou = 0\n        count = 0\n        logged_images = False\n        \n        # Evaluate and save test results\n        with torch.no_grad():\n            for images, targets, paths in test_loader:\n                images, targets = images.to(device), targets.to(device)\n                d_output = model(images)\n                \n                mean_iou = calculate_iou_and_save(d_output[0], targets, paths, results_folder)\n                total_iou += mean_iou\n                count += 1\n                \n                save_images(d_output[0], paths, results_folder)\n                \n                # Log hình ảnh đầu ra vào wandb (chỉ log 1 ảnh đầu tiên để tránh nặng)\n                if not logged_images:\n                    pred_img = d_output[0][0][0].detach().cpu().numpy()\n                    true_mask = targets[0][0].detach().cpu().numpy()\n                    wandb.log({\n                        \"Prediction\": wandb.Image(pred_img, caption=\"Predicted mask\"),\n                        \"Target\": wandb.Image(true_mask, caption=\"Ground Truth\")\n                    })\n                    logged_images = True\n        # Log IoU trung bình toàn bộ test set sau mỗi epoch\n        wandb.log({\"val_mean_iou\": total_iou / count})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T09:49:15.694586Z","iopub.execute_input":"2025-04-22T09:49:15.695386Z","iopub.status.idle":"2025-04-22T11:23:13.755810Z","shell.execute_reply.started":"2025-04-22T09:49:15.695355Z","shell.execute_reply":"2025-04-22T11:23:13.754842Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Kết thúc\nwandb.finish()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T11:28:10.445280Z","iopub.execute_input":"2025-04-22T11:28:10.445868Z","iopub.status.idle":"2025-04-22T11:28:10.449101Z","shell.execute_reply.started":"2025-04-22T11:28:10.445848Z","shell.execute_reply":"2025-04-22T11:28:10.448368Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from PIL import Image\nimport torch\nimport torchvision.transforms as transforms\n\n# 1. Load ảnh\nimg_path = '/kaggle/input/testte1/cju7ey10f2rvf0871bwbi9x82.jpg'\nimage = io.imread(img_path)[:, :, :3]\nimage = transform.resize(image, (512, 512), mode='constant') / np.max(image)\nimage = (image - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])\n\n\ninput_tensor = torch.tensor(image, dtype=torch.float).permute(2, 0, 1)\ninput_tensor = input_tensor.unsqueeze(0)\n# 3. Load model\nmodel = U2NET()  # hoặc model phù hợp\nif torch.cuda.device_count() > 1:\n    model = torch.nn.DataParallel(model)\nmodel = model.to(device)\n\nmodel_path = '/kaggle/input/testte1/u2net_50.pth'\nmodel.load_state_dict(torch.load(model_path))\nmodel.eval()\n\n# 4. Predict\nwith torch.no_grad():\n    input_tensor = input_tensor.to(device)\n    output = model(input_tensor)\n    prediction = output[0]  # lấy output đầu tiên nếu model trả nhiều đầu ra\n\n# 5. Xử lý output: chuyển về numpy để lưu hoặc vẽ\npred_mask = prediction[0][0].cpu().numpy()  # lấy batch_idx=0 và channel_idx=0\n\n# Nếu muốn lưu mask ra ảnh:\nimport matplotlib.pyplot as plt\nplt.imsave('predicted_mask1.png', pred_mask, cmap='gray')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T09:01:53.130533Z","iopub.execute_input":"2025-05-01T09:01:53.131065Z","iopub.status.idle":"2025-05-01T09:01:55.248874Z","shell.execute_reply.started":"2025-05-01T09:01:53.131045Z","shell.execute_reply":"2025-05-01T09:01:55.248302Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/2614144450.py:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(model_path))\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"from skimage import io, transform\nimport numpy as np\nimport torch\n\ndef preprocess_single_image(image_path, resize=512):\n    \"\"\"\n    Load and preprocess a single image for prediction.\n    Same preprocessing as SegmentationDataset.\n    \"\"\"\n    image = io.imread(image_path)[:, :, :3]  # đảm bảo ảnh 3 channels\n    image = transform.resize(image, (resize, resize), mode='constant') / np.max(image)  # resize và scale về [0,1]\n\n    # Chuẩn hóa giống lúc training\n    image = (image - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])\n\n    tensor_image = torch.tensor(image, dtype=torch.float).permute(2, 0, 1)  # (C, H, W)\n    tensor_image = tensor_image.unsqueeze(0)  # (1, C, H, W) thêm batch dimension\n    return tensor_image\n\ndef predict_single_image(model, image_path, device, resize=512):\n    \"\"\"\n    Predict a single image.\n    \"\"\"\n    model.eval()\n\n    input_tensor = preprocess_single_image(image_path, resize=resize)\n    input_tensor = input_tensor.to(device)\n\n    with torch.no_grad():\n        d_output = model(input_tensor)\n        prediction = d_output[0]  # lấy output đầu tiên nếu model trả nhiều outputs\n\n    # Normalize prediction\n    normalized_pred = normalize_prediction(prediction)\n\n    return normalized_pred[0][0].cpu().numpy()  # trả về mask (H, W)\n\n# Ví dụ sử dụng:\n# model đã load từ .pth và move về device trước rồi\nimg_path = '/kaggle/input/testte/cju0s690hkp960855tjuaqvv0.jpg'\npredicted_mask = predict_single_image(model, img_path, device=device, resize=512)\n\n# Lưu mask ra file hoặc visualize:\nimport matplotlib.pyplot as plt\nplt.imsave('predicted_mask3.png', predicted_mask, cmap='gray')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T09:33:14.709403Z","iopub.execute_input":"2025-04-28T09:33:14.710182Z","iopub.status.idle":"2025-04-28T09:33:14.922218Z","shell.execute_reply.started":"2025-04-28T09:33:14.710158Z","shell.execute_reply":"2025-04-28T09:33:14.921458Z"}},"outputs":[],"execution_count":9}]}